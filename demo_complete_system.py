#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ AI ê°ì²´ ìƒì„¸ ë¶„ì„ ì‹œìŠ¤í…œ ì™„ì „ ë°ëª¨
í°íŠ¸ í’ˆì§ˆ ê°œì„  + AI API ë¶„ì„ í†µí•© ì‹œìŠ¤í…œ
"""

import cv2
import numpy as np
import time
import os
from ultralytics import YOLO
from ui_design_improved import ImprovedUIDesign
from ai_object_analyzer import AIObjectAnalyzer
import argparse

class CompleteSystemDemo:
    """ì™„ì „í•œ AI ê°ì²´ ë¶„ì„ ì‹œìŠ¤í…œ ë°ëª¨"""
    
    def __init__(self):
        # UI ë””ìì¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print("ğŸ¨ ê°œì„ ëœ UI ë””ìì¸ ì‹œìŠ¤í…œ ë¡œë“œ...")
        self.ui_design = ImprovedUIDesign()
        
        # AI ë¶„ì„ê¸° ì´ˆê¸°í™”
        print("ğŸ¤– AI ê°ì²´ ìƒì„¸ ë¶„ì„ê¸° ë¡œë“œ...")
        try:
            self.ai_analyzer = AIObjectAnalyzer()
            self.use_ai_analysis = True
            print("âœ… AI ë¶„ì„ê¸° ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            self.ai_analyzer = None
            self.use_ai_analysis = False
            print(f"âš ï¸ AI ë¶„ì„ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        print("ğŸ” YOLO11 ëª¨ë¸ ë¡œë“œ...")
        self.model = YOLO('yolo11n.pt')
        self.model.conf = 0.5
        print("âœ… YOLO11 Nano ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ë°ëª¨ ì„¤ì •
        self.tracked_objects = {}
        self.next_id = 1
        self.frame_count = 0
        self.ai_analysis_interval = 5  # 5í”„ë ˆì„ë§ˆë‹¤ AI ë¶„ì„
        
    def create_demo_image(self):
        """ë°ëª¨ìš© ì´ë¯¸ì§€ ìƒì„± (ê°€ìƒ ê°ì²´ë“¤)"""
        # 800x600 ê²€ì€ ë°°ê²½
        demo_image = np.zeros((600, 800, 3), dtype=np.uint8)
        
        # ë°°ê²½ ê·¸ë¼ë°ì´ì…˜
        for i in range(600):
            ratio = i / 600
            color = int(30 + ratio * 20)
            cv2.line(demo_image, (0, i), (800, i), (color, color, color), 1)
        
        # ê°€ìƒ ê°ì²´ë“¤ ê·¸ë¦¬ê¸°
        objects = [
            {"name": "person", "box": [100, 150, 250, 400], "conf": 0.85},
            {"name": "car", "box": [400, 250, 650, 450], "conf": 0.92},
            {"name": "cell phone", "box": [200, 100, 280, 160], "conf": 0.78},
            {"name": "laptop", "box": [300, 350, 500, 450], "conf": 0.88},
        ]
        
        # ê°ì²´ ìœ¤ê³½ì„  ê·¸ë¦¬ê¸°
        colors = [(52, 152, 219), (46, 204, 113), (231, 76, 60), (155, 89, 182)]
        for i, obj in enumerate(objects):
            x1, y1, x2, y2 = obj["box"]
            color = colors[i % len(colors)]
            
            # ê°ì²´ ì˜ì—­ ì±„ìš°ê¸° (ë°˜íˆ¬ëª…)
            overlay = demo_image.copy()
            cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
            cv2.addWeighted(demo_image, 0.7, overlay, 0.3, 0, demo_image)
            
            # í…Œë‘ë¦¬
            cv2.rectangle(demo_image, (x1, y1), (x2, y2), color, 2)
            
            # ì¤‘ì•™ì— ê°ì²´ëª… í‘œì‹œ
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2
            text = f"{obj['name']}"
            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
            text_x = center_x - text_size[0] // 2
            text_y = center_y + text_size[1] // 2
            
            cv2.putText(demo_image, text, (text_x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return demo_image, objects
    
    def simulate_detection(self, objects):
        """ì‹¤ì œ YOLO ê²€ì¶œ ê²°ê³¼ë¥¼ ì‹œë®¬ë ˆì´ì…˜"""
        detections = []
        for obj in objects:
            detection_data = {
                'box': obj['box'],
                'class': obj['name'],
                'confidence': obj['conf']
            }
            detections.append(detection_data)
        return detections
    
    def update_tracked_objects(self, detections):
        """ì¶”ì  ê°ì²´ ì—…ë°ì´íŠ¸"""
        for i, detection in enumerate(detections):
            obj_id = i + 1
            if obj_id not in self.tracked_objects:
                self.tracked_objects[obj_id] = {
                    'box': detection['box'],
                    'class': detection['class'],
                    'confidence': detection['confidence'],
                    'stable_count': 1,
                    'avg_confidence': detection['confidence'],
                    'total_frames': 1,
                }
            else:
                # ê¸°ì¡´ ê°ì²´ ì—…ë°ì´íŠ¸
                self.tracked_objects[obj_id]['stable_count'] += 1
                self.tracked_objects[obj_id]['total_frames'] += 1
                frames = self.tracked_objects[obj_id]['total_frames']
                old_avg = self.tracked_objects[obj_id]['avg_confidence']
                new_conf = detection['confidence']
                self.tracked_objects[obj_id]['avg_confidence'] = (old_avg * (frames-1) + new_conf) / frames
    
    def add_ai_analysis(self, obj_data):
        """AI ë¶„ì„ ì •ë³´ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ API ì—†ì„ ë•Œ)"""
        class_name = obj_data['class']
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ AI ë¶„ì„ ê²°ê³¼
        ai_analysis_samples = {
            'person': {
                'brand': 'Human',
                'model': 'Adult Male',
                'type': 'Standing Person',
                'color': 'Blue Shirt',
                'condition': 'Active',
                'confidence': 0.89
            },
            'car': {
                'brand': 'Toyota',
                'model': 'Camry 2023',
                'type': 'Sedan',
                'color': 'Silver',
                'condition': 'Good',
                'confidence': 0.92
            },
            'cell phone': {
                'brand': 'Apple',
                'model': 'iPhone 14 Pro',
                'type': 'Smartphone',
                'color': 'Space Gray',
                'condition': 'Excellent',
                'confidence': 0.87
            },
            'laptop': {
                'brand': 'MacBook',
                'model': 'MacBook Pro M2',
                'type': 'Laptop Computer',
                'color': 'Space Gray',
                'condition': 'New',
                'confidence': 0.91
            }
        }
        
        if class_name in ai_analysis_samples:
            analysis = ai_analysis_samples[class_name]
            obj_data['ai_analysis'] = analysis
            obj_data['detailed_name'] = f"{analysis['brand']} {analysis['model']}"
        
        return obj_data
    
    def run_demo(self, mode='static'):
        """ë°ëª¨ ì‹¤í–‰"""
        print("ğŸš€" + "="*60)
        print("ğŸ¯ AI ê°ì²´ ìƒì„¸ ë¶„ì„ ì‹œìŠ¤í…œ ì™„ì „ ë°ëª¨")
        print("="*60)
        print(f"ğŸ¨ ê°œì„ ëœ í°íŠ¸ ì‹œìŠ¤í…œ: âœ…")
        print(f"ğŸ¤– AI ìƒì„¸ ë¶„ì„ ì‹œìŠ¤í…œ: {'âœ…' if self.use_ai_analysis else 'âŒ'}")
        print(f"ğŸ“Š YOLO11 ê°ì²´ ê°ì§€: âœ…")
        print("")
        
        if mode == 'static':
            # ì •ì  ì´ë¯¸ì§€ ë°ëª¨
            print("ğŸ“¸ ì •ì  ì´ë¯¸ì§€ ë°ëª¨ ëª¨ë“œ")
            demo_image, objects = self.create_demo_image()
            detections = self.simulate_detection(objects)
            self.update_tracked_objects(detections)
            
            # AI ë¶„ì„ ì¶”ê°€
            for obj_id, obj_data in self.tracked_objects.items():
                if self.frame_count % self.ai_analysis_interval == 0:
                    obj_data = self.add_ai_analysis(obj_data)
            
            # UI ì˜¤ë²„ë ˆì´ ê·¸ë¦¬ê¸°
            for obj_id, obj_data in self.tracked_objects.items():
                self.ui_design.draw_modern_info_card(demo_image, obj_id, obj_data)
            
            # ì •ë³´ íŒ¨ë„ ì¶”ê°€
            tracker_info = {
                'model_name': 'ğŸš€ YOLO11 Nano',
                'fps': 30.0,
                'object_count': len(self.tracked_objects),
                'accuracy': 87.5,
                'stable_objects': len(self.tracked_objects),
                'model_params': '2.6M'
            }
            
            final_image = self.ui_design.draw_modern_info_panel(demo_image, tracker_info)
            
            # ê²°ê³¼ í‘œì‹œ
            cv2.namedWindow('ğŸš€ AI ê°ì²´ ìƒì„¸ ë¶„ì„ ì‹œìŠ¤í…œ ë°ëª¨', cv2.WINDOW_NORMAL)
            cv2.resizeWindow('ğŸš€ AI ê°ì²´ ìƒì„¸ ë¶„ì„ ì‹œìŠ¤í…œ ë°ëª¨', 1200, 800)
            cv2.imshow('ğŸš€ AI ê°ì²´ ìƒì„¸ ë¶„ì„ ì‹œìŠ¤í…œ ë°ëª¨', final_image)
            
            print("ğŸ’¡ í‚¤ë³´ë“œ ì¡°ì‘:")
            print("  - 's': ìŠ¤í¬ë¦°ìƒ· ì €ì¥")
            print("  - 'q': ì¢…ë£Œ")
            print("")
            
            while True:
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s'):
                    timestamp = int(time.time())
                    filename = f'ai_analysis_demo_{timestamp}.jpg'
                    cv2.imwrite(filename, final_image)
                    print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filename}")
            
            cv2.destroyAllWindows()
        
        else:
            # ë™ì  ë°ëª¨ (ì• ë‹ˆë©”ì´ì…˜)
            print("ğŸ¬ ë™ì  ì• ë‹ˆë©”ì´ì…˜ ë°ëª¨ ëª¨ë“œ")
            # í–¥í›„ êµ¬í˜„ ê°€ëŠ¥
            
        print("ğŸš€ ë°ëª¨ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='AI ê°ì²´ ìƒì„¸ ë¶„ì„ ì‹œìŠ¤í…œ ì™„ì „ ë°ëª¨')
    parser.add_argument('--mode', choices=['static', 'dynamic'], default='static',
                        help='ë°ëª¨ ëª¨ë“œ ì„ íƒ')
    args = parser.parse_args()
    
    # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸
    print("ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸...")
    
    required_files = [
        'ui_design_improved.py',
        'ai_object_analyzer.py',
        'yolo11n.pt'
    ]
    
    missing_files = []
    for file in required_files:
        if not os.path.exists(file):
            missing_files.append(file)
    
    if missing_files:
        print(f"âŒ í•„ìˆ˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_files}")
        return
    
    print("âœ… ëª¨ë“  í•„ìˆ˜ íŒŒì¼ í™•ì¸ë¨")
    
    # ë°ëª¨ ì‹¤í–‰
    demo = CompleteSystemDemo()
    demo.run_demo(args.mode)

if __name__ == "__main__":
    main()
